Scientific workflows are a means of defining and orchestrating large, complex, multi-stage computations that perform data analysis and simulation. 
%Task clustering is a task granularity optimization technique that merges multiple short workflow tasks into a single job such that the scheduling overheads and communication cost are reduced and the overall runtime performance is significantly improved. 
Many scientific workflows often involve a large amount of data transfers and computations and require efficient optimization techniques to reduce the runtime of execution. 
Today, with the emergence of executing large-scale scientific workflows in modern distributed environments such as grids and clouds, the optimization of workflow runtime has introduced new challenges that existing methods cannot tackle. Traditionally, many existing methods are confined to the task scheduling problem, without due consideration of the refinement of workflow structures, system overheads, and failure occurrence. Refining workflow structures using techniques such as workflow partitioning and task clustering represent a new trend of runtime optimization of great benefits. 
For example, the runtime improvement of these workflow restructuring methods depends on the ratio of application computation to system overheads, which has more benefits in modern distributed systems that often involve multiple distant components. 

This thesis argues that workflow restructuring techniques can significantly improve the performance of executing scientific workflows in modern distributed environments. In particular, we improve the quality of workflow partitioning and task clustering techniques that are more popularly used. Several previous studies also utilize workflow partitioning and task clustering techniques to improve the runtime of executing scientific workflows. However, existing methods perform in an empirical way and require users' knowledge to tune the performance. For example, many workflow partitioning techniques do not consider constraints on resources such as the data storage. Also, many task clustering methods optimize task granularity at the workflow level without consideration on data dependencies between tasks. We distinguish our work by modeling a practical distributed system with imbalanced load and transient failures. 
%Task granularity optimization is a key problem in the execution of workflows because they often involve large overheads and computations that must be optimized. 
%However, the recent emergence of executing large-scale scientific workflows on modern distributed environments, such as grids and clouds, brings new challenges to the optimization of task granularity and requires novel mechanisms and new knowledge in several aspects. 
We investigate the key concern of refining workflow structures and propose a series of innovative workflow partitioning and task clustering methods to improve the runtime performance. Simulation-based and real system-based evaluation verifies the effectiveness of our methods. 


