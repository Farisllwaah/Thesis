Scientific workflows are a means of defining and orchestrating large, complex, multi-stage computations that perform data analysis and simulation. Task clustering is a runtime optimization technique that merges multiple short workflow tasks into a single job such that the scheduling overheads and communication cost are reduced and the overall runtime performance is significantly improved. However, the recent emergence of large-scale scientific workflows executing on modern distributed environments, such as grids and clouds, requires a new methodology that considers task clustering in a comprehensive way. Our work investigates the key concern of workflow studies and proposes a series of dynamic methods to improve the overall workflow performance, including runtime performance, fault tolerance, and resource utilization. Simulation-based and experiment-based evaluation verifies the effectiveness of our methods. 


