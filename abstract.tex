Scientific workflows are a means of defining and orchestrating large, complex, multi-stage computations that perform data analysis and simulation. 
%Task clustering is a task granularity optimization technique that merges multiple short workflow tasks into a single job such that the scheduling overheads and communication cost are reduced and the overall runtime performance is significantly improved. 
Task granularity optimization is a key problem in the execution of workflows because they often involve large overheads and computations that must be optimized. 
Traditionally, optimizing task granularity at the workload sphere without consideration of data dependencies has been widely discussed in the literature. 
However, the recent emergence of executing large-scale scientific workflows on modern distributed environments, such as grids and clouds, brings new challenges to the optimization of task granularity and requires novel mechanisms and new knowledge in several aspects. Our work investigates the key concern of task granularity studies in scientific workflows and proposes a series of innovative partitioning and clustering methods to improve the overall workflow performance, including runtime performance, fault tolerance, and data locality. Simulation-based and experiment-based evaluation verifies the effectiveness of our approaches. 


