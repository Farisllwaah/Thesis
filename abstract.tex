Scientific workflows are a means of defining and orchestrating large, complex, multi-stage computations that perform data analysis and simulation. 
%Task clustering is a task granularity optimization technique that merges multiple short workflow tasks into a single job such that the scheduling overheads and communication cost are reduced and the overall runtime performance is significantly improved. 
Many of these scientific workflows often involve a large amount of data transfer and computations and require efficient optimization techniques to reduce the runtime of execution. 
Today, with the emergence of executing large-scale scientific workflows in modern distributed environments such as grids and clouds, the optimization of workflow runtime has introduced new challenges that existing methods cannot satisfy. Traditionally, many existing methods considered the allocation of computation resources and the scheduling of tasks, but did not effectively take into account the refinement of workflow structures, system overheads, and failure occurrence. Refining workflow structures using techniques such as workflow partitioning and task clustering represent a new trend of runtime optimization of great benefits. 
For example, the runtime improvement of these workflow restructuring methods depends on the ratio of application computation to system overheads, which is more promising in modern distributed systems that often involve multiple distant components. 

This thesis states that workflow restructuring techniques can significantly improve the performance of executing scientific workflows in modern distributed environments. In particular, we improve the quality of workflow partitioning and task clustering techniques that are more popularly used. Many other studies also utilize workflow partitioning and task clustering techniques to improve the runtime of executing scientific workflows. However, existing methods perform in an empirical way and require users' knowledge to tune the performance. For example, many workflow partitioning techniques do not consider constraints on resources such as the data storage. Also, many task clustering methods optimize task granularity at the workflow level without consideration on data dependencies between tasks. We also distinguish our work by modeling a practical distributed system with imbalanced load and transient failures. 
%Task granularity optimization is a key problem in the execution of workflows because they often involve large overheads and computations that must be optimized. 
%However, the recent emergence of executing large-scale scientific workflows on modern distributed environments, such as grids and clouds, brings new challenges to the optimization of task granularity and requires novel mechanisms and new knowledge in several aspects. 
Our work investigates the key concern of refining workflow structures and proposes a series of innovative workflow partitioning and task clustering methods to improve the overall workflow performance, including runtime performance, fault tolerance, and data locality. Simulation-based and experiment-based evaluation verifies the effectiveness of our methods. 


