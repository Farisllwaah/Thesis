\chapter{Resource Aware Clustering} 


Nowadays, with the emergence of cloud computing and resource provisioning, we are able to dynamically allocate resources into multiple execution sites or hosts. Such a new approach requires us to consider the task clustering problem and resource provisioning problem together so as to minimize both the workflow execution time (makespan) and the monetary cost of the workflow execution (resource cost).  


\section{Related Work}

\textbf{Resource Allocation.} A decentralized resource allocation policy in minigrid is presented in \cite{Yang2007}. It is shown that the traditional resource allocating policies statically assign resources to the jobs according to the distribution schema computed by the job scheduling policy. Those policies cannot handle the DAGs with large jobs that could be represented by services. Decentralized resource allocating policy is proposed to solve this problem by sharing the computing burden on several processors. However the existing decentralized policies cannot dynamically compute the dependent relationships of a given DAG or even schedule the data driven jobs. Ranjan \cite{Ranjan2008} proposed a decentralized and cooperative workflow scheduling algorithm. The proposed approach utilizes a Peer-to-Peer (P2P) coordination space with respect to coordinating the application schedules among the Grid wide distributed workflow brokers. 


In highly heterogeneous and distributed systems, like Grids, it is rather difficult to provide QoS to the users. Although the grid provides ready access to large pools of computational resources, the traditional approach to accessing these resources suffers from many overheads that lead to poor performance. Below we examine several techniques that are used to reduce these overheads. 

%\textbf{Resource Utlization}
%\cite{Tomas2012}  As reservations of resources may not always be possible, another possible way of enhancing the perceived QoS is by performing meta-scheduling of jobs in advance, where jobs are scheduled some time before they are actually executed. Thank to this, it is more likely that the appropriate resources are available to execute the job when needed. When using this type of scheduling, fragmentation appears and may become the cause of poor resource utilization. Because of that, some techniques are needed to perform rescheduling of tasks that may reduce the existing fragmentation. To this end, knowing the status of the system is a must. However, how to measure and quantify the existing fragmentation in a Grid system is a challenging task. This paper proposes different metrics aiming at measuring that fragmentation not only at resource level but also taking into account all the resources of the Grid environment as a whole. Finally, a performance evaluation of the proposed metrics over a real testbed is presented.



\textbf{Advance Reservation.} Advanced reservation support in distributed resource allocation is desirable for performance predictability, meeting resource requirements, and providing Quality of Service (QoS) guarantees \cite{Castillo2008, Foster1999, Mcgough2005, Meinl2008}.  Elmroth \cite{Elmroth2009} proposed an algorithm to perform resource selection based on performance predictions and provided support for advance reservations. This work also provided an algorithm for moving already made reservations by making coallocation of jobs. However, advance resource reservations is not always possible in all the Grid resources. For instance, in the desktops resources, since the local users has more priority to run their jobs.

\textbf{Resource Provisionining.} With a provisioning tool \cite{Juve2010a}, resources are allocated for the exclusive use of a single user for a given period of time. This minimizes queuing delays because the userâ€™s jobs no longer compete with other jobs for access to resources. 
Villegas \cite{Villegas2012} presented a comprehensive and empirical performance-cost analysis of provisioning and allocation policies in IaaS clouds. They introduced a taxonomy of both types of policies, based on the type of information used in the decision process, and mapped to this taxonomy eight provisioning and four allocation policies. However, resource provisioning involves some management challenges and administers of a grid may not favor such an approach. 

\textbf{Task Reallocation.} With the aim of dynamically balancing the computational load among resources, some jobs have to be moved from one resource to another and/or from one period of time to another, which is called task reallocation \cite{Tomas2012}. Caniou  \cite{Caniou2011} presented a reallocation mechanism that tunes parallel jobs each time they are submitted to the local resource manager (which implies also each time a job is migrated). They only needed to query batch schedulers with simple submission or cancellation requests.  Its authors also presented different reallocation algorithms and studied their behaviors in the case of a multi-cluster Grid environment. In \cite{Zhang2000}, a pre-emptive process migration method was proposed to dynamically migrate processes from overloaded nodes to lightly-loaded nodes. However, it can achieve a well balance only when there are some idle nodes (e.g. when the number of task processes is less than that of nodes). In our case with large scale scientific workflows, usually we have more tasks than available nodes. 

%The problem of how to perform task clustering in terms of a diamond workflow has been solved by arbitrary clustering with the assumption that the number of resources is known. However, in modern distributed systems such as clouds, the number of available resources is infinite compared to the requirement of a workflow. A user has to pay the resources for what has been used. With the emergence of resource provisioning \cite{Deelman2006, Juve2008, Juve2010a} on clouds and grids, we are able to allocate dedicated resources before the computation ever starts. However, the resource utilization remains an open problem, which aims to reduce the resource waste while at the same time the workflow performance is not damaged. For example, in order to efficiently use petascale systems such as XSEDE \cite{XSEDE} for large workflow applications, Pegasus uses an MPI-based task management tool called pegasus-mpi-cluster (PMC). Using MPI enables us to leverage the underlying network communications libraries through a common interface that is portable across cutting edge cyberinfrastructure. In MPI-based task management systems, task clustering enables us to assign an arbitrary clustered job to one cluster and in this job may have internal data dependencies. 


%A PMC job consists of a single master process (this process is rank 0 in MPI parlance) and several worker processes. These processes follow the standard master-worker architecture. The master process manages the workflow and assigns workflow tasks to workers for execution. The workers execute the tasks and return the results to the master. Communication between the master and the workers is accomplished using a simple text-based protocol implemented using MPI\_Send and MPI\_Recv. 

\section{Resource Aware and Utilization Oriented Clustering}

Below we propose a resource aware clustering that integrates task clustering and resource allocation such that resource utilization can be improved. The challenge in resource aware clustering is how to estimate the resource cost and overall runtime of workflow together. Once it is done, the optimal clustering strategies can be solved by our former methods. 

We assume that we use static provisioning, which means the resources are available before the computation ever starts and do not change until the computation stops. 


\section{Planned Work}
In the future, we are going to develop resource aware clustering methods and discuss its runtime performance and overall resource utilization. 

The first goal is to determine if we can predict the runtime of a real workflow with enough accuracy to be useful based on only predictions of the total runtime, critical path, max width, and number of cores. The disadvantage of many traditional workflow estimation models is that they involves assumptions and requires tuning many parameters, which is not feasible in practice. Therefore, in our work, we aim to use as few parameters as possible and assure the model is simple but effective in practice. 

Once we have answered that question, we can use the model we developed to create a clustering algorithm for resource provisioning tools, which involves partitioning the workflow and predicting the number of cores and wall time required for each partition. We also aim to look at a workflow and develop an estimate that can be used to provision VMs on a cloud (i.e., Amazon EC2) and predict the cost of running the workflow. That would be very useful for developing provisioning algorithms with deadline and budget constraints.

Another concern we will look at is the probability of under/over-predicting the workflow wall time. What we would like to have is an estimate of the wall time that is greater than the actual wall time 95\% of the time. It would be easy to achieve 100\% by just doubling or tripling a crude estimate, but that is not useful. For this it would be useful to get a sense of how much the total wall time of a workflow varies over repeated runs of the same workflow.

One further problem with the experiment we are about to undertake is that we are probably going to assume we have perfect estimates, at least initially. In other words, we are going to use the actual runtime of the jobs from the experiment to build a model that predicts the wall time of the experiment. For example, the easiest way to get the critical path of the clustered workflow would be to run the experiment on 256 cores and see what wall time we get. In doing that we are basically assuming we have a perfect estimate. It would be interesting to take our perfect estimate and make it imperfect by increasing amounts to see how our prediction changes. We thus can answer a question: How accurate do our job runtimes need to be in order to get a workflow wall time prediction with a given accuracy?

Finally, consider what would happen if, instead of a single estimate of the runtime of each task, we had a statistical distribution of the historical runtimes for similar tasks. We are working toward building a database of this type of information in dV/dT for runtime, memory, and I/O.

