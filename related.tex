\chapter{Related Work}
\label{chap:related}






\section{Workflow Modeling and Analysis}

\textbf{Scientific Workflows} continue to gain in popularity among many science disciplines, including physics \cite{Deelman2002}, astronomy \cite{Sakellariou2010}, biology \cite{Lathers2006, Oinn2004}, chemistry \cite{Wieczorek2005}, earthquake science \cite{Maechling2007} and many more. A scientific workflow is a high-level specification of a set of tasks needed to manage a computational science or engineering process and the dependencies between them that must be satisfied in order to accomplish a specific goal. Today, scientific workflows increasingly require tremendous amounts of data processing and workflows with up to a few million tasks are not uncommon \cite{Callaghan2011}. For example, the Montage Galactic Plane workflow~\cite{Berriman2004} has about 18 million input images that sum up to 2.5TB data and 10.5 million computational tasks that uses 34,000 CPU hours. The CyberShake workflow~\cite{Callaghan2008} is a software platform that uses 3D waveform modeling to perform probabilistic seismic hazard analysis. CyberShake has about 840,000 task executions that are required for computing the hazard for a single geographical site, while the targeted southern California area has about 200 geographical sites. In this thesis, we use Montage and CyberShake along with other popular scientific workflows to evaluate our proposed methods. 

\textbf{Workflow Management Systems} (WMS) such as Askalon \cite{Wieczorek2005}, Kepler~\cite{kepler}, Taverna \cite{Oinn2004}, and Pegasus \cite{Deelman2004} are designed to run scientific workflows in distributed environments. 
Askalon \cite{Fahringer2005} provides a suite of middleware services that support the execution of scientific workflows on distributed infrastructures. Askalon workflows are written in an XML-based workflow language that supports many different looping and branching structures. These workflows are interpreted to coordinate data flow and task execution on distributed infrastructures including grids, clusters and clouds.
Kepler~\cite{kepler} is a graphical system for designing, executing, reusing, evolving, archiving, and sharing scientific workflows. In Kepler, workflows are created by connecting a series of workflow components called Actors, through which data are processed. Each Actor has several Ports through which input and output Tokens containing data and data references are sent and received. Each workflow has a Director that determines the model of computation used by the workflow, and Kepler supports several Directors with different execution semantics, including Synchronous Data Flow and Process Network directors.
Taverna~\cite{Oinn2004} copes with an environment of autonomous service provides and allows easy access and composition of as wide a range of services as feasible. A Taverna workflow specification is compiled into a multi-threaded object model, where processors are represented by objects, and data transfers from the output port of one processor to the input port of a downstream processor are realized using local method invocations between processor objects. One or more activities are associated to each processor. 
Pegasus \cite{Deelman2004}, which stands for Planning for Execution in Grids, was developed at the USC Information Sciences Institute as part of the GriPhyN \cite{Deelman2002} and SCEC/IT \cite{Maechling2007} projects. Pegasus receives an abstract workflow description in a XML format from users, produces a concrete or executable workflow, and submits it to DAGMan for execution. For the workflow partitioning techniques, we use Pegasus to execute a sub-workflow in an execution site. 

\textbf{Workflow Modeling} Many workflow systems use a particular workflow language or representation (BPEL \cite{BPEL}, Petri Nets~\cite{alt2006grid}, SCUFL \cite{Oinn2004}, DAGMan \cite{Kalayci2010}, DAX \cite{Deelman2005}), which has a specification that can be composed by hand using a plain text editor. BPEL (Business Process Execution Language) \cite{BPEL} is the de facto standard for Web-service-based workflows with a number of implementations from business as well as open source organizations. Petri Nets~\cite{alt2006grid} is an established tool for modeling and analyzing distributed processes in business as well as the IT sector. Petri Nets can be used to express complex workflows, e.g. loops. 
DAGs (Directed Acyclic Graphs)~\cite{Deelman2005} are one of the task graph representations that are widely used as the programming model in many workflow management systems such as Pegasus~\cite{Deelman2004} and DAGwoman~\cite{Tschager2012DAGwoman}. DAGs are easy to use and intuitive: each node in the DAG represents a workflow task, and the edges represent dependencies between the tasks that constrain the order in which the tasks are executed. 
%Moreover, algorithms expressed as DAGs have the potential to alleviate the user from focusing on the architectural issues, while allowing the engine to extract the best performance from the underlying architecture. 
In this work, we extend the DAG model to be overhead-aware, which is more realistic and helps us model the process of task clustering in scientific workflows in a more comprehensive way. 
%Among these workflow management systems, Pegasus~\cite{Singh2008} has implemented and used a basic algorithm of task clustering called Horizontal Clustering (HC) that merges task at the same horizontal levels of the workflow. The clustering granularity (number of tasks within a cluster) of a clustered job is controlled by the user, who defines either the number of tasks per clustered job (\emph{clusters.size}), or the number of clustered jobs per horizontal level of the workflow (\emph{clusters.num}). We further extend HC to consider data transfer cost, the balancing between dependencies and computation, and the failure occurrence. 




\textbf{Workflow Patterns and Characteristics}~\cite{Juve2013, Liu2008} are used to capture and abstract the common structure within a workflow and they give insights on designing new workflows and optimization methods. Juve et al.~\cite{Juve2013} provided a characterization of workflow from 6 scientific applications and obtained task-level performance metrics (I/O, CPU, and memory consumption). They also presented an execution profile for each workflow running at a typical scale and managed by the Pegasus workflow management system~\cite{Deelman2005}. Liu et al.~\cite{Liu2008} used time-series segmentation to discover the smallest pattern sets and 
predicted the activity duration with pattern matching results. We illustrate the relationship between the workflow patterns (asymmetric or symmetric workflows) and the performance of our balancing algorithms. 
Some work in literature has further attempted to define and model workflow characteristics with quantitative metrics. In~\cite{Ali2004}, the authors proposed a robustness metric for resource allocation in parallel and distributed systems and accordingly customized the definition of robustness. Tolosana et al.~\cite{Tolosana2011} defined a metric called Quality of Resilience to assess how resilient workflow enactment is likely to be in the presence of failures. Ma et al. ~\cite{Ma:2014:GDB:2560969.2561388} proposed a graph distance based metric for measuring the similarity between data-intensive workflows with variable time constraints, where a formal structure called time dependency graph (TDG) is proposed. Similarity comparison between two workflows can be reduced to computing the similarity between TDGs. Based on their work, we focus on novel quantitative metrics such as a similar distance metric that are able to demonstrate the imbalance problem in scientific workflows. 

%Imbalance paper
\textbf{Overhead Analysis}~\cite{Ostberg2011, Prodan2008, Chen2011} is a topic of great interest in the Grid community. Stratan et al.~\cite{Stratan2008} evaluate overheads in a real-world execution environment including DAGMan/Condor and Karajan/Globus. Their methodology focuses on five system characteristics: overhead, raw performance, stability, scalability, and reliability. They pointed out that overheads in head nodes should not be ignored and that the main bottleneck in a busy system is often the head node. Prodan et al.~\cite{Prodan2008} offered a complete Grid workflow overhead classification and a systematic measurement of overheads. In Chen et al.~\cite{Chen2011}, we extended that work by providing a measurement of major overheads imposed by workflow management systems and execution environments and analyzed how existing optimization techniques improve workflow runtime by reducing or overlapping overheads during execution. The existence of system overheads is an important reason why task clustering can provide significant performance improvement for workflow-based applications. In this thesis, we aim to further improve the performance of task clustering techniques using using information about system overheads. 


\section{Workflow Paritioning}



\textbf{Workflow Partitioning.} Because of the limited resources at an execution site, scientists execute scientific workflows \cite{Bharathi2008, Rubing2005} in distributed large-scale computational environments such as multi-cluster grids, that is, grids comprising multiple independent execution sites. Topcuoglu \cite{Topcuoglu2002} presented a classification of widely used task scheduling approaches. Such scheduling solutions, however, cannot be applied directly to multi-cluster grids. First, the data transfer delay between multiple execution sites is more significant than that within an execution site and thus it should be considered. Second, they do not consider the dynamic resource availability in grids, which also makes accurate predictions of computation and communication costs difficult. Sonmez \cite{Sonmez2010} extended the traditional scheduling problem to multiple workflows on multi-cluster grids and presented a performance of a wide range of dynamic workflow scheduling policies in multi-cluster grids. Duan \cite{Rubing2005} and Wieczorek \cite{Wieczorek2005} have discussed the scheduling and the partitioning of scientific workflows in dynamic grids in the presence of a broad set of unpredictable overheads and possible failures. Duan \cite{Rubing2005} then developed a distributed service-oriented Enactment Engine with a master-slave architecture for de-centralized coordination of scientific workflows. Kumar \cite{Kumar2002} proposed the use of graph partitioning to divide the resources of a distributed system, but not the workflow itself, which means the resources are provisioned from different execution sites but the workflows are not partitioned at all. Dong \cite{Dong2007} and Kalayci \cite{Kalayci2010} have discussed the use of graph partitioning algorithms for workflows according to resource requirements of the workflow tasks and the availability of selected clusters. Our work focuses on the workflow partitioning problem with resource constraints, in particular, the data storage constraint. Compared to Dong \cite{Dong2007} and Kalayci \cite{Kalayci2010}, we extend their work to estimate the overall runtime of sub-workflows and then schedule these sub-workflows based on the estimates. 



\textbf{Data Placement} techniques try to strategically manage placement of data before or during the execution of a workflow. Kosar et al. \cite{Kosar2004} presented Stork, a scheduler for data placement activities on grids and proposed to make data placement activities as first class citizens in the Grid. In Stork, data placement is a job and is decoupled from computational jobs. Amer et al. \cite{Amer2012} studied the relationship between data placement services and workflow management systems for data-intensive applications. They proposed an asynchronous mode of data placement in which data placement operations are performed as data sets become available and according to the policies of the virtual organization and not according to the directives of the workflow management system (WMS). The WMS can however assist the placement services with the staging of data based on information collected during task executions and data transfers. Shankar \cite{Shankar2007} presented an architecture for Condor in which the input, output and executable files of jobs are cached on the local disks of the machines in a cluster. Caching can reduce the amount of pipelines and batch I/O that is transferred across the network. This in turn significantly reduces the response time for workflows with data-intensive workloads. In contrast, we mainly focus on the workflow partitioning and task clustering problem but our work can be extended to consider the proposed data placement strategies. 



\textbf{Data Throttling.} Park et al. \cite{Humphrey2008} limits the amount of parallel data transfer to avoid overloading supporting services such as data servers, which is called data throttling. Throttling is especially useful for unbalanced workflows in which one task might be idle while waiting for data to arrive. However, as discussed in \cite{Humphrey2008}, data throttling has an impact on the overall workflow performance depending on the ratio between computational and data transfer tasks. Therefore, performance analysis is necessary after the profiling of data transfers so that the relationship between computation and data transfers can be identified more explicitly. Rodríguez \cite{Rodríguez2012} proposed an automated and trace-based workflow structural analysis method for DAGs. Files transfers are completed as fast as the network bandwidth allows, and once transferred, the files are buffered/stored at their destination. To improve the use of network bandwidth and buffer/storage within a workflow, they adjusted the speeds of some data transfers and assured that tasks have all their input data arriving at the same time. Compared to our work, data throttling has a limit in performance gain by the amount of data transfer that can be reduced, while our partitioning approach can improve the overall workflow runtime and resource usage. 







\section{Task Clustering}

\textbf{Workflow Scheduling}. There have been a considerable amount of work trying to solve workflow-mapping problem using DAG scheduling heuristics such as HEFT \cite{Topcuoglu2002}, Min-Min \cite{Blythe2005}, MaxMin \cite{Braun2001}, MCT \cite{Braun2001}, etc. Duan \cite{Rubing2005} and Wieczorek \cite{Wieczorek2005} have discussed the scheduling and the partitioning of scientific workflows in dynamic grids. The emergence of cloud computing \cite{Armbrust2009} has made it possible to easily lease large-scale homogeneous computing resources from commercial resource providers and satisfy QoS requirements. In our case, since we assume resources are homogeneous, the resource selection is not a major concern and thus the scheduling problem can be simplified as a task clustering problem. 
More specifically, a plethora of  scheduling algorithms have been developed in the networking and operating system domains. Many of these schedulers have been extended to perform in a hierarchical fashion~\cite{Lifflander2012}. Lifflander et al.~\cite{Lifflander2012} proposed to use work stealing and a hierarchical persistence-based rebalancing algorithm to address the imbalance problem in scheduling. Zheng et al.~\cite{Zheng2011} presented an automatic hierarchical load balancing method that overcomes the scalability challenges of centralized schemes and poor solutions of traditional distributed schemes. There are other scheduling algorithms~\cite{Braun2001} (e.g. list scheduling) that indirectly achieve load balancing of workflows through makespan minimization. However, the benefit that can be achieved through traditional scheduling optimization is limited by its complexity. The performance gain of task clustering is primarily determined by the ratio between system overheads and task runtime, which is more substantial in modern distributed systems such as clouds and grids. 

\textbf{Task Granularity Control} has also been addressed in scientific workflows. For instance, Singh et al.~\cite{Singh2008} proposed a level- and label-based clustering. In level-based clustering, tasks at the same workflow level can be clustered together. The number of clusters or tasks per cluster is specified by the user. In the label-based clustering, the user labels tasks that should be clustered together. Although their work considers data dependencies between workflow levels, it is done manually by the users, a process that is prone to errors. Recently, Ferreira da Silva et al.~\cite{Ferreira-granularity-2013} proposed task grouping and ungrouping algorithms to control workflow task granularity in a non-clairvoyant and online context, where none or few characteristics about the application or resources are known in advance. Their work significantly reduced scheduling and queuing time overheads, but did not consider data dependencies. The low performance of \emph{fine-grained} tasks is a common problem in widely distributed platforms where the scheduling overhead and queuing times at resources are high, such as Grid and Cloud systems. Several works have addressed the control of task granularity of bags of tasks. For instance, Muthuvelu et al.~\cite{Muthuvelu2005} proposed a clustering algorithm that groups bags of tasks based on their runtime---tasks are grouped up to the resource capacity. Later, they extended their work~\cite{4493929} to determine task granularity based on task file size, CPU time, and resource constraints. Recently, they proposed an online scheduling algorithm~\cite{Muthuvelu2010,Muthuvelu2013} that groups tasks based on resource network utilization, user's budget, and application deadline. Ng et al.~\cite{Keat2006} and Ang et al.~\cite{Ang2009} introduced bandwidth in the scheduling framework to enhance the performance of task scheduling. Longer tasks are assigned to resources with better bandwidth. Liu and Liao~\cite{Liu2009} proposed an adaptive fine-grained job scheduling algorithm to group fine-grained tasks according to processing capacity and bandwidth of the current available resources. Although these techniques significantly reduce the impact of scheduling and queuing time overhead, they did not consider data dependencies.


\textbf{Failure Analysis and Modeling} \cite{Tang1990} presents system characteristics such as error and failure distribution and hazard rates. They have emphasized the importance of fault tolerant design and concluded that the failure rates in modern distributed systems should not be ignored. Schroeder et al. \cite{Schroeder2006} has studied the statistics of the data, including the root cause of failures, the mean time between failures, and the mean time to repair.  They have verified the inter-arrival time of task failures fits a Weibull distribution better than lognormal and exponential distribution. Sahoo et al. \cite{Sahoo2004} analyzed the empirical and statistical properties of system errors and failures from a network of heterogeneous servers. Their results concluded that the system error and failure patterns are comprised of time-varying behavior containing long stationary intervals.
Oppenheimer et al. \cite{Oppenheimer2002} analyzed the causes of failures from three large-scale Internet services and they found that configuration error was the largest single cause of failures in these services. McConnel \cite{McConnel1979} analyzed the transient errors in computer systems and showed that transient errors follow a Weibull distribution. In \cite{Sun2003, Iosup2008}, the Weibull distribution is one of the best fit for the workflow traces they used.  Similarly, we measure the inter-arrival time of failures in a workflow and we further provide methods to improve task clustering.  
An increasing number of workflow management systems are taking fault tolerance into consideration. The Pegasus workflow management system \cite{Deelman2004} has incorporated a task-level monitoring system and used job retries to address the issue of task failures. They also used provenance data to track the failure records and analyzed the causes of failures \cite{Samak2011}. Plankensteiner et. al. \cite{plankensteiner2009fault} have surveyed fault detection, prevention and recovery techniques in current Grid workflow management systems such as Askalon \cite{fahringer2007askalon}, Chemomentum \cite{schuller2008chemomentum}, Escogitare \cite{laforenza2007biological} and Triana \cite{taylor2007triana}. Recovery techniques such as replication, checkpointing, task resubmission, and task migration etc. have been used in these systems. We are integrating the work on failure analysis with the optimization in task clustering. To be best of our knowledge, none of existing workflow management systems have provided such features. 

\textbf{Load Balancing.} With the aim of dynamically balancing the computational load among resources, some jobs have to be moved from one resource to another and/or from one period of time to another, which is called task reallocation \cite{Tomas2012}. Caniou  \cite{Caniou2011} presented a reallocation mechanism that tunes parallel jobs each time they are submitted to the local resource manager (which implies also each time a job is migrated). They only needed to query batch schedulers with simple submission or cancellation requests.  Its authors also presented different reallocation algorithms and studied their behavior in the case of a multi-cluster Grid environment. In \cite{Zhang2000}, a preemptive process migration method was proposed to dynamically migrate processes from overloaded computational nodes to lightly-loaded nodes. However, this approach can achieve a good balance only when there are some idle compute nodes (e.g. when the number of task processes is less than that of compute nodes). In our case, since we deal with large-scale scientific workflows, usually we have more tasks than available compute nodes. 
Guo et al. \cite{Zhenhua2011} presented mechanisms to dynamically split and consolidate tasks to cope with load balancing. They have proposed algorithms to dynamically split unfinished tasks to fill idle compute nodes. Similarly, Ying et al. \cite{Ying2009} proposed a load-balancing algorithm based on collaborative task clustering. The algorithm divides the collaborative computing tasks into subtasks and then dynamically allocates them to the servers. Compared to these approaches, our work selects tasks to be merged based on their task runtime distribution and their data dependencies initially, without introducing additional overheads during the runtime. Also, a quantitative approach of imbalance measurement guides the selection of tasks to be merged without causing runtime imbalance or dependency imbalance. 



\textbf{Machine Learning in Workflow Optimization} has been used to predict execution time \cite{Rubing2009, 1015660, 1542747, da2013toward} and system overheads~\cite{Chen2011}, and develop probability distributions for transient failure characteristics. Duan et.al. \cite{Rubing2009} used Bayesian network to model and predict workflow task runtimes. The important attributes (such as the external load, arguments etc. ) are dynamically selected by the Bayesian network and fed into a radial basis function neural network to make further predictions. Ferreira da Silva et. al. \cite{da2013toward} used regression trees to dynamically estimate task behavior including process I/O, runtime, memory peak and disk usage. Samak~\cite{samak2012failure} modeled failure characteristics observed during the execution of large scientific workflows on Amazon EC2 and used a Naive Bayes classifier to accurately predict the failure probability of jobs. Their results show that job type is the most significant factor to classify failures. 
We reuse the knowledge gained from prior work on failure analysis, overhead analysis and task runtime analysis. We then use prior knowledge based on Maximum Likelihood Estimation to integrate both the knowledge and runtime feedbacks and adjust the estimation accordingly. 



